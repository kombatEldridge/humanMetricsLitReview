@inproceedings{shalyminov-etal-2018-neural,
    title = "Neural Response Ranking for Social Conversation: A Data-Efficient Approach",
    author = "Shalyminov, Igor  and Du{\v{s}}ek, Ond{\v{r}}ej  and Lemon, Oliver",
    editor = "Chuklin, Aleksandr  and Dalton, Jeff  and Kiseleva, Julia  and Borisov, Alexey  and Burtsev, Mikhail",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI}",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5701",
    doi = "10.18653/v1/W18-5701",
    pages = "1--8",
    abstract = "The overall objective of {`}social{'} dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics, including social chit-chat. Apart from raw dialogue data, user-provided ratings are the most common signal used to train such systems to produce engaging responses. In this paper we show that social dialogue systems can be trained effectively from raw unannotated data. Using a dataset of real conversations collected in the 2017 Alexa Prize challenge, we developed a neural ranker for selecting {`}good{'} system responses to user utterances, i.e. responses which are likely to lead to long and engaging conversations. We show that (1) our neural ranker consistently outperforms several strong baselines when trained to optimise for user ratings; (2) when trained on larger amounts of data and only using conversation length as the objective, the ranker performs better than the one trained using ratings {--} ultimately reaching a Precision@1 of 0.87. This advance will make data collection for social conversational agents simpler and less expensive in the future.",
}

@inproceedings{ding-etal-2023-enhancing,
    title = "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    author = "Ding, Ning  and Chen, Yulin  and Xu, Bokai  and Qin, Yujia  and Hu, Shengding  and Liu, Zhiyuan  and Sun, Maosong  and Zhou, Bowen",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.183",
    doi = "10.18653/v1/2023.emnlp-main.183",
    pages = "3029--3051",
    abstract = "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models.",
}
@inproceedings{firdaus-etal-2020-incorporating,
    title = "Incorporating Politeness across Languages in Customer Care Responses: Towards building a Multi-lingual Empathetic Dialogue Agent",
    author = "Firdaus, Mauajama  and Ekbal, Asif  and Bhattacharyya, Pushpak",
    editor = "Calzolari, Nicoletta  and B{\'e}chet, Fr{\'e}d{\'e}ric  and Blache, Philippe  and Choukri, Khalid  and Cieri, Christopher  and Declerck, Thierry  and Goggi, Sara  and Isahara, Hitoshi  and Maegaard, Bente  and Mariani, Joseph  and Mazo, H{\'e}l{\`e}ne  and Moreno, Asuncion  and Odijk, Jan  and Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.514",
    pages = "4172--4182",
    abstract = "Customer satisfaction is an essential aspect of customer care systems. It is imperative for such systems to be polite while handling customer requests/demands. In this paper, we present a large multi-lingual conversational dataset for English and Hindi. We choose data from Twitter having both generic and courteous responses between customer care agents and aggrieved users. We also propose strong baselines that can induce courteous behaviour in generic customer care response in a multi-lingual scenario. We build a deep learning framework that can simultaneously handle different languages and incorporate polite behaviour in the customer care agent{'}s responses. Our system is competent in generating responses in different languages (here, English and Hindi) depending on the customer{'}s preference and also is able to converse with humans in an empathetic manner to ensure customer satisfaction and retention. Experimental results show that our proposed models can converse in both the languages and the information shared between the languages helps in improving the performance of the overall system. Qualitative and quantitative analysis shows that the proposed method can converse in an empathetic manner by incorporating courteousness in the responses and hence increasing customer satisfaction.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{chen-etal-2023-generate,
    title = "Generate-then-Retrieve: Intent-Aware {FAQ} Retrieval in Product Search",
    author = "Chen, Zhiyu  and Choi, Jason  and Fetahu, Besnik  and Rokhlenko, Oleg  and Malmasi, Shervin",
    editor = "Sitaram, Sunayana  and Beigman Klebanov, Beata  and Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.73",
    doi = "10.18653/v1/2023.acl-industry.73",
    pages = "763--771",
    abstract = "Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user{'}s shopping experience poses challenges on deciding when and how to show FAQ results. Our proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. Offline evaluation demonstrates that our approach improves 12{\%} in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95{\%} compared to baseline systems. These improvements are further validated by real user feedback, where more than 99{\%} of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale.",
}
@inproceedings{firdaus-etal-2022-polise,
    title = "{P}oli{S}e: Reinforcing Politeness Using User Sentiment for Customer Care Response Generation",
    author = "Firdaus, Mauajama  and Ekbal, Asif  and Bhattacharyya, Pushpak",
    editor = "Calzolari, Nicoletta  and Huang, Chu-Ren  and Kim, Hansaem  and Pustejovsky, James  and Wanner, Leo  and Choi, Key-Sun  and Ryu, Pum-Mo  and Chen, Hsin-Hsi  and Donatelli, Lucia  and Ji, Heng  and Kurohashi, Sadao  and Paggio, Patrizia  and Xue, Nianwen  and Kim, Seokhwan  and Hahm, Younggyun  and He, Zhong  and Lee, Tony Kyungil  and Santus, Enrico  and Bond, Francis  and Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.538",
    pages = "6165--6175",
    abstract = "The interaction between a consumer and the customer service representative greatly contributes to the overall customer experience. Therefore, to ensure customers{'} comfort and retention, it is important that customer service agents and chatbots connect with users on social, cordial, and empathetic planes. In the current work, we automatically identify the sentiment of the user and transform the neutral responses into polite responses conforming to the sentiment and the conversational history. Our technique is basically a reinforced multi-task network- the primary task being {`}polite response generation{'} and the secondary task being {`}sentiment analysis{'}- that uses a Transformer based encoder-decoder. We use sentiment annotated conversations from Twitter as the training data. The detailed evaluation shows that our proposed approach attains superior performance compared to the baseline models.",
}
@inproceedings{malkin-etal-2022-coherence,
    title = "Coherence boosting: When your pretrained language model is not paying enough attention",
    author = "Malkin, Nikolay  and Wang, Zhen  and Jojic, Nebojsa",
    editor = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.565",
    doi = "10.18653/v1/2022.acl-long.565",
    pages = "8214--8236",
    abstract = "Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM{'}s focus on a long context. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.",
}
@inproceedings{mimno-etal-2011-optimizing,
    title = "Optimizing Semantic Coherence in Topic Models",
    author = "Mimno, David  and Wallach, Hanna  and Talley, Edmund  and Leenders, Miriam  and McCallum, Andrew",
    editor = "Barzilay, Regina  and Johnson, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1024",
    pages = "262--272",
}
@inproceedings{brunato-etal-2023-coherent,
    title = "Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages",
    author = "Brunato, Dominique  and Dell{'}Orletta, Felice  and Dini, Irene  and Ravelli, Andrea Amelio",
    editor = "Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.680",
    doi = "10.18653/v1/2023.findings-acl.680",
    pages = "10690--10700",
    abstract = "In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages. We employ a consistent evaluation framework to compare the performance of monolingual and multilingual models in both in-domain and out-domain settings. Additionally, we explore the model{'}s performance in a cross-language scenario.",
}
@inproceedings{xenos-etal-2021-context,
    title = "Context Sensitivity Estimation in Toxicity Detection",
    author = "Xenos, Alexandros  and Pavlopoulos, John  and Androutsopoulos, Ion",
    editor = "Mostafazadeh Davani, Aida  and Kiela, Douwe  and Lambert, Mathias  and Vidgen, Bertie  and Prabhakaran, Vinodkumar  and Waseem, Zeerak",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.15",
    doi = "10.18653/v1/2021.woah-1.15",
    pages = "140--145",
    abstract = "User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new dataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.",
}
@inproceedings{markl-lai-2021-context,
    title = "Context-sensitive evaluation of automatic speech recognition: considering user experience {\&} language variation",
    author = "Markl, Nina  and Lai, Catherine",
    editor = "Blodgett, Su Lin  and Madaio, Michael  and O'Connor, Brendan  and Wallach, Hanna  and Yang, Qian",
    booktitle = "Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.hcinlp-1.6",
    pages = "34--40",
    abstract = "Commercial Automatic Speech Recognition (ASR) systems tend to show systemic predictive bias for marginalised speaker/user groups. We highlight the need for an interdisciplinary and context-sensitive approach to documenting this bias incorporating perspectives and methods from sociolinguistics, speech {\&} language technology and human-computer interaction in the context of a case study. We argue evaluation of ASR systems should be disaggregated by speaker group, include qualitative error analysis, and consider user experience in a broader sociolinguistic and social context.",
}
@inproceedings{wiegmann-etal-2022-language,
    title = "Language Models as Context-sensitive Word Search Engines",
    author = {"Wiegmann, Matti  and Volske, Michael  and Stein, Benno  and Potthast, Martin"},
    editor = "Huang, Ting-Hao 'Kenneth'  and Raheja, Vipul  and Kang, Dongyeop  and Chung, John Joon Young  and Gissin, Daniel  and Lee, Mina  and Gero, Katy Ilonka",
    booktitle = "Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.in2writing-1.5",
    doi = "10.18653/v1/2022.in2writing-1.5",
    pages = "39--45",
    abstract = "Context-sensitive word search engines are writing assistants that support word choice, phrasing, and idiomatic language use by indexing large-scale n-gram collections and implementing a wildcard search. However, search results become unreliable with increasing context size (e.g., n{\textgreater}=5), when observations become sparse. This paper proposes two strategies for word search with larger n, based on masked and conditional language modeling. We build such search engines using BERT and BART and compare their capabilities in answering English context queries with those of the n-gram-based word search engine Netspeak. Our proposed strategies score within 5 percentage points MRR of n-gram collections while answering up to 5 times as many queries.",
}
@inproceedings{shen-etal-2018-learning,
    title = "Learning Context-Sensitive Convolutional Filters for Text Processing",
    author = "Shen, Dinghan  and Min, Martin Renqiang  and Li, Yitong  and Carin, Lawrence",
    editor = "Riloff, Ellen  and Chiang, David  and Hockenmaier, Julia  and Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1210",
    doi = "10.18653/v1/D18-1210",
    pages = "1839--1848",
    abstract = "Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-sensitive filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.",
}
@inproceedings{sordoni-etal-2015-neural,
    title = "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
    author = "Sordoni, Alessandro  and Galley, Michel  and Auli, Michael  and Brockett, Chris  and Ji, Yangfeng  and Mitchell, Margaret  and Nie, Jian-Yun  and Gao, Jianfeng  and Dolan, Bill",
    editor = "Mihalcea, Rada  and Chai, Joyce  and Sarkar, Anoop",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-1020",
    doi = "10.3115/v1/N15-1020",
    pages = "196--205",
}
@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and Singh, Amanpreet  and Michael, Julian  and Hill, Felix  and Levy, Omer  and Bowman, Samuel",
    editor = "Linzen, Tal  and Chrupa{\l}a, Grzegorz  and Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
@inproceedings{jiao-etal-2020-tinybert,
    title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
    author = "Jiao, Xiaoqi  and Yin, Yichun  and Shang, Lifeng  and Jiang, Xin  and Chen, Xiao  and Li, Linlin  and Wang, Fang  and Liu, Qun",
    editor = "Cohn, Trevor  and He, Yulan  and Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.372",
    doi = "10.18653/v1/2020.findings-emnlp.372",
    pages = "4163--4174",
    abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}
@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and Williams, Adina  and Dinan, Emily  and Bansal, Mohit  and Weston, Jason  and Kiela, Douwe",
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.441",
    doi = "10.18653/v1/2020.acl-main.441",
    pages = "4885--4901",
    abstract = "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
}
@inproceedings{liu-etal-2019-multi,
    title = "Multi-Task Deep Neural Networks for Natural Language Understanding",
    author = "Liu, Xiaodong  and He, Pengcheng  and Chen, Weizhu  and Gao, Jianfeng",
    editor = "Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1441",
    doi = "10.18653/v1/P19-1441",
    pages = "4487--4496",
    abstract = "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{\%} (2.2{\%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
}
@inproceedings{hamalainen-alnajjar-2021-human,
    title = "Human Evaluation of Creative {NLG} Systems: An Interdisciplinary Survey on Recent Papers",
    author = {"H{\"a}m{\"a}l{\"a}inen, Mika  and Alnajjar, Khalid"},
    editor = "Bosselut, Antoine  and Durmus, Esin  and Gangal, Varun Prashant  and Gehrmann, Sebastian  and Jernite, Yacine  and Perez-Beltrachini, Laura  and Shaikh, Samira  and Xu, Wei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.9",
    doi = "10.18653/v1/2021.gem-1.9",
    pages = "84--95",
    abstract = "We survey human evaluation in papers presenting work on creative natural language generation that have been published in INLG 2020 and ICCC 2020. The most typical human evaluation method is a scaled survey, typically on a 5 point scale, while many other less common methods exist. The most commonly evaluated parameters are meaning, syntactic correctness, novelty, relevance and emotional value, among many others. Our guidelines for future evaluation include clearly defining the goal of the generative system, asking questions as concrete as possible, testing the evaluation setup, using multiple different evaluation setups, reporting the entire evaluation process and potential biases clearly, and finally analyzing the evaluation results in a more profound way than merely reporting the most typical statistics.",
}
@inproceedings{khullar-etal-2018-automatic,
    title = "Automatic Question Generation using Relative Pronouns and Adverbs",
    author = "Khullar, Payal  and Rachna, Konigari  and Hase, Mukul  and Shrivastava, Manish",
    editor = "Shwartz, Vered  and Tabassum, Jeniya  and Voigt, Rob  and Che, Wanxiang  and de Marneffe, Marie-Catherine  and Nissim, Malvina",
    booktitle = "Proceedings of {ACL} 2018, Student Research Workshop",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-3022",
    doi = "10.18653/v1/P18-3022",
    pages = "153--158",
    abstract = "This paper presents a system that automatically generates multiple, natural language questions using relative pronouns and relative adverbs from complex English sentences. Our system is syntax-based, runs on dependency parse information of a single-sentence input, and achieves high accuracy in terms of syntactic correctness, semantic adequacy, fluency and uniqueness. One of the key advantages of our system, in comparison with other rule-based approaches, is that we nearly eliminate the chances of getting a wrong wh-word in the generated question, by fetching the requisite wh-word from the input sentence itself. Depending upon the input, we generate both factoid and descriptive type questions. To the best of our information, the exploitation of wh-pronouns and wh-adverbs to generate questions is novel in the Automatic Question Generation task.",
}
@inproceedings{bi-etal-2020-knowledge,
    title = "Knowledge-enriched, Type-constrained and Grammar-guided Question Generation over Knowledge Bases",
    author = "Bi, Sheng  and Cheng, Xiya  and Li, Yuan-Fang  and Wang, Yongzhen  and Qi, Guilin",
    editor = "Scott, Donia  and Bel, Nuria  and Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.250",
    doi = "10.18653/v1/2020.coling-main.250",
    pages = "2776--2786",
    abstract = "Question generation over knowledge bases (KBQG) aims at generating natural-language questions about a subgraph, i.e. a set of triples. Two main challenges still face the current crop of encoder-decoder-based methods, especially on small subgraphs: (1) low diversity and poor fluency due to the limited information contained in the subgraphs, and (2) semantic drift due to the decoder{'}s oblivion of the semantics of the answer entity. We propose an innovative knowledge-enriched, type-constrained and grammar-guided KBQG model, named KTG, to addresses the above challenges. In our model, the encoder is equipped with auxiliary information from the KB, and the decoder is constrained with word types during QG. Specifically, entity domain and description, as well as relation hierarchy information are considered to construct question contexts, while a conditional copy mechanism is incorporated to modulate question semantics according to current word types. Besides, a novel reward function featuring grammatical similarity is designed to improve both generative richness and syntactic correctness via reinforcement learning. Extensive experiments show that our proposed model outperforms existing methods by a significant margin on two widely-used benchmark datasets SimpleQuestion and PathQuestion.",
}
@inproceedings{katinskaia-yangarber-2021-assessing,
    title = "Assessing Grammatical Correctness in Language Learning",
    author = "Katinskaia, Anisia  and Yangarber, Roman",
    editor = "Burstein, Jill  and Horbach, Andrea  and Kochmar, Ekaterina  and Laarmann-Quante, Ronja  and Leacock, Claudia  and Madnani, Nitin  and Pil{\'a}n, Ildik{\'o}  and Yannakoudakis, Helen  and Zesch, Torsten",
    booktitle = "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bea-1.15",
    pages = "135--146",
    abstract = "We present experiments on assessing the grammatical correctness of learners{'} answers in a language-learning System (references to the System, and the links to the released data and code are withheld for anonymity). In particular, we explore the problem of detecting alternative-correct answers: when more than one inflected form of a lemma fits syntactically and semantically in a given context. We approach the problem with the methods for grammatical error detection (GED), since we hypothesize that models for detecting grammatical mistakes can assess the correctness of potential alternative answers in a learning setting. Due to the paucity of training data, we explore the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments show a. that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; b. that fine-tuned BERT yields promising results on assessing the correctness of grammatical exercises; and c. establish a new benchmark for Russian. To further investigate its performance, we compare fine-tuned BERT with one of the state-of-the-art models for GED (Bell et al., 2019) on our dataset and RULEC-GEC (Rozovskaya and Roth, 2019). We release the manually annotated learner dataset, used for testing, for general use.",
}
@inproceedings{ren-etal-2019-generating,
    title = "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency",
    author = "Ren, Shuhuai  and Deng, Yihe  and He, Kun  and Che, Wanxiang",
    editor = "Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1103",
    doi = "10.18653/v1/P19-1103",
    pages = "1085--1097",
    abstract = "We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.",
}
@inproceedings{belkhir-sadat-2023-beyond,
    title = "Beyond Information: Is {C}hat{GPT} Empathetic Enough?",
    author = "Belkhir, Ahmed  and Sadat, Fatiha",
    editor = "Mitkov, Ruslan  and Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.18",
    pages = "159--169",
    abstract = "This paper aims to explore and enhance ChatGPT{'}s abilities to generate more human-like conversations by taking into account the emotional state of the user. To achieve this goal, a prompt-driven Emotional Intelligence is used through the empathetic dialogue dataset in order to propose a more empathetic conversational language model. We propose two altered versions of ChatGPT as follows: (1) an emotion-infused version which takes the user{'}s emotion as input before generating responses using an emotion classifier based on ELECTRA ; and (2) the emotion adapting version that tries to accommodate for how the user feels without any external component. By analyzing responses of the two proposed altered versions and comparing them to the standard version of ChatGPT, we find that using the external emotion classifier leads to more frequent and pronounced use of positive emotions compared to the standard version. On the other hand, using simple prompt engineering to take the user emotion into consideration, does the opposite. Finally, comparisons with state-of-the-art models highlight the potential of prompt engineering to enhance the emotional abilities of chatbots based on large language models.",
}
@inproceedings{li-etal-2022-continuing,
    title = "Continuing Pre-trained Model with Multiple Training Strategies for Emotional Classification",
    author = "Li, Bin  and Weng, Yixuan  and Song, Qiya  and Sun, Bin  and Li, Shutao",
    editor = "Barnes, Jeremy  and De Clercq, Orph{\'e}e  and Barriere, Valentin  and Tafreshi, Shabnam  and Alqahtani, Sawsan  and Sedoc, Jo{\~a}o  and Klinger, Roman  and Balahur, Alexandra",
    booktitle = "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wassa-1.22",
    doi = "10.18653/v1/2022.wassa-1.22",
    pages = "233--238",
    abstract = "Emotion is the essential attribute of human beings. Perceiving and understanding emotions in a human-like manner is the most central part of developing emotional intelligence. This paper describes the contribution of the LingJing team{'}s method to the Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis (WASSA) 2022 shared task on Emotion Classification. The participants are required to predict seven emotions from empathic responses to news or stories that caused harm to individuals, groups, or others. This paper describes the continual pre-training method for the masked language model (MLM) to enhance the DeBERTa pre-trained language model. Several training strategies are designed to further improve the final downstream performance including the data augmentation with the supervised transfer, child-tuning training, and the late fusion method. Extensive experiments on the emotional classification dataset show that the proposed method outperforms other state-of-the-art methods, demonstrating our method{'}s effectiveness. Moreover, our submission ranked Top-1 with all metrics in the evaluation phase for the Emotion Classification task.",
}
@inproceedings{kumar-etal-2023-multilingual,
    title = "From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues",
    author = "Kumar, Shivani  and S, Ramaneswaran  and Akhtar, Md  and Chakraborty, Tanmoy",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.598",
    doi = "10.18653/v1/2023.emnlp-main.598",
    pages = "9638--9652",
    abstract = "Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC.",
}
@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and Rashkin, Hannah  and Chen, Derek  and Le Bras, Ronan  and Choi, Yejin",
    editor = "Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}
@inproceedings{liu-etal-2021-modulating,
    title = "Modulating Language Models with Emotions",
    author = "Liu, Ruibo  and Wei, Jason  and Jia, Chenyan  and Vosoughi, Soroush",
    editor = "Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.379",
    doi = "10.18653/v1/2021.findings-acl.379",
    pages = "4332--4339",
}
@inproceedings{caron-srivastava-2023-manipulating,
    title = "Manipulating the Perceived Personality Traits of Language Models",
    author = "Caron, Graham  and Srivastava, Shashank",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.156",
    doi = "10.18653/v1/2023.findings-emnlp.156",
    pages = "2370--2386",
    abstract = "Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the {`}Big Five{'}. Categorizations like the {`}Big Five{'} are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it{'}s perceived {`}Big Five{'} personality traits. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also show that when exposed to different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 consistently identify and mirror personality markers in those contexts. This behavior illustrates an ability to be manipulated in a predictable way (with correlations up to 0.84 between intended and realized changes in personality traits), and frames them as tools for controlling personas in applications such as dialog systems. We contribute two data-sets of personality descriptions of humans subjects.",
}
@inproceedings{oraby-etal-2018-controlling,
    title = "Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators",
    author = "Oraby, Shereen  and Reed, Lena  and Tandon, Shubhangi  and T.S., Sharath  and Lukin, Stephanie  and Walker, Marilyn",
    editor = "Komatani, Kazunori  and Litman, Diane  and Yu, Kai  and Papangelis, Alex  and Cavedon, Lawrence  and Nakano, Mikio",
    booktitle = "Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5019",
    doi = "10.18653/v1/W18-5019",
    pages = "180--190",
    abstract = "Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated semantics. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three models. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.",
}
@inproceedings{ramos-etal-2018-building,
    title = "Building a Corpus for Personality-dependent Natural Language Understanding and Generation",
    author = "Ramos, Ricelli  and Neto, Georges  and Silva, Barbara  and Monteiro, Danielle  and Paraboni, Ivandr{\'e}  and Dias, Rafael",
    editor = "Calzolari, Nicoletta  and Choukri, Khalid  and Cieri, Christopher  and Declerck, Thierry  and Goggi, Sara  and Hasida, Koiti  and Isahara, Hitoshi  and Maegaard, Bente  and Mariani, Joseph  and Mazo, H{\'e}l{\`e}ne  and Moreno, Asuncion  and Odijk, Jan  and Piperidis, Stelios  and Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1183",
}
@inproceedings{mairesse-walker-2007-personage,
    title = "{PERSONAGE}: Personality Generation for Dialogue",
    author = "Mairesse, Fran{\c{c}}ois  and Walker, Marilyn",
    editor = "Zaenen, Annie  and van den Bosch, Antal",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-1063",
    pages = "496--503",
}
@inproceedings{chen-soo-2018-humor,
    title = "Humor Recognition Using Deep Learning",
    author = "Chen, Peng-Yu  and Soo, Von-Wun",
    editor = "Walker, Marilyn  and Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2018",
    doi = "10.18653/v1/N18-2018",
    pages = "113--117",
    abstract = "Humor is an essential but most fascinating element in personal communication. How to build computational models to discover the structures of humor, recognize humor and even generate humor remains a challenge and there have been yet few attempts on it. In this paper, we construct and collect four datasets with distinct joke types in both English and Chinese and conduct learning experiments on humor recognition. We implement a Convolutional Neural Network (CNN) with extensive filter size, number and Highway Networks to increase the depth of networks. Results show that our model outperforms in recognition of different types of humor with benchmarks collected in both English and Chinese languages on accuracy, precision, and recall in comparison to previous works.",
}
@inproceedings{yang-etal-2015-humor,
    title = "Humor Recognition and Humor Anchor Extraction",
    author = "Yang, Diyi  and Lavie, Alon  and Dyer, Chris  and Hovy, Eduard",
    editor = "M{\`a}rquez, Llu{\'\i}s  and Callison-Burch, Chris  and Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1284",
    doi = "10.18653/v1/D15-1284",
    pages = "2367--2376",
}
@inproceedings{shani-etal-2021-get,
    title = "{H}ow Did This Get Funded?! {A}utomatically Identifying Quirky Scientific Achievements",
    author = "Shani, Chen  and Borenstein, Nadav  and Shahaf, Dafna",
    editor = "Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.2",
    doi = "10.18653/v1/2021.acl-long.2",
    pages = "14--28",
    abstract = "Humor is an important social phenomenon, serving complex social and psychological functions. However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: {``}Are cows more likely to lie down the longer they stand?{''}). This challenging task has unique characteristics that make it particularly suitable for automatic learning. We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP. We use our models to identify potentially funny papers in a large dataset of over 630,000 articles. The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines",
}
@inproceedings{novikova-etal-2018-rankme,
    title = "{R}ank{ME}: Reliable Human Ratings for Natural Language Generation",
    author = "Novikova, Jekaterina  and Du{\v{s}}ek, Ond{\v{r}}ej  and Rieser, Verena",
    editor = "Walker, Marilyn  and Ji, Heng  and Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2012",
    doi = "10.18653/v1/N18-2012",
    pages = "72--78",
    abstract = "Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.",
}
@inproceedings{higashinaka-etal-2010-modeling,
    title = "Modeling User Satisfaction Transitions in Dialogues from Overall Ratings",
    author = "Higashinaka, Ryuichiro  and Minami, Yasuhiro  and Dohsaka, Kohji  and Meguro, Toyomi",
    editor = "Katagiri, Yasuhiro  and Nakano, Mikio",
    booktitle = "Proceedings of the {SIGDIAL} 2010 Conference",
    month = sep,
    year = "2010",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W10-4304",
    pages = "18--27",
}
@inproceedings{husse-spitz-2022-mind,
    title = "Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models",
    author = "Husse, Silke  and Spitz, Andreas",
    editor = "Goldberg, Yoav  and Kozareva, Zornitsa  and Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.311",
    doi = "10.18653/v1/2022.findings-emnlp.311",
    pages = "4212--4234",
    abstract = "The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. However, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsistent, and ultimately inconclusive. To address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. Our results show that minor design and implementation decisions (or errors) have a substantial and often significant impact on the derived bias scores. Overall, we find the state of the field to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. Based on our findings, we conclude with a discussion of paths towards more robust and consistent bias detection methods.",
}
@inproceedings{koksal-etal-2023-language,
    title = "Language-Agnostic Bias Detection in Language Models with Bias Probing",
    author = {"K{\"o}ksal, Abdullatif  and Yalcin, Omer  and Akbiyik, Ahmet  and Kilavuz, M.  and Korhonen, Anna  and Schuetze, Hinrich"},
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.848",
    doi = "10.18653/v1/2023.findings-emnlp.848",
    pages = "12735--12747",
    abstract = "Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases. Quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. To address this, we propose a bias probing technique called LABDet, for evaluating social bias in PLMs with a robust and language-agnostic method. For nationality as a case study, we show that LABDet {``}surfaces{''} nationality bias by training a classifier on top of a frozen PLM on non-nationality sentiment detection. We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context. We also show for English BERT that bias surfaced by LABDet correlates well with bias in the pretraining data; thus, our work is one of the few studies that directly links pretraining data to PLM behavior. Finally, we verify LABDet{'}s reliability and applicability to different templates and languages through an extensive set of robustness checks. We publicly share our code and dataset in https://github.com/akoksal/LABDet.",
}
@inproceedings{ramesh-etal-2023-fairness,
    title = "Fairness in Language Models Beyond {E}nglish: Gaps and Challenges",
    author = "Ramesh, Krithika  and Sitaram, Sunayana  and Choudhury, Monojit",
    editor = "Vlachos, Andreas  and Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.157",
    doi = "10.18653/v1/2023.findings-eacl.157",
    pages = "2106--2119",
    abstract = "With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. In this paper, we survey different aspects of fairness in languages beyond English and multilingual contexts. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.",
}
@inproceedings{cao-etal-2022-intrinsic,
    title = "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
    author = "Cao, Yang Trista  and Pruksachatkun, Yada  and Chang, Kai-Wei  and Gupta, Rahul  and Kumar, Varun  and Dhamala, Jwala  and Galstyan, Aram",
    editor = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.62",
    doi = "10.18653/v1/2022.acl-short.62",
    pages = "561--570",
    abstract = "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
}
@inproceedings{delobelle-etal-2022-measuring,
    title = "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
    author = "Delobelle, Pieter  and Tokpo, Ewoenam  and Calders, Toon  and Berendt, Bettina",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.122",
    doi = "10.18653/v1/2022.naacl-main.122",
    pages = "1693--1706",
    abstract = "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify {`}bias{'} and {`}fairness{'} in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
}
@inproceedings{han-etal-2022-systematic,
    title = "Systematic Evaluation of Predictive Fairness",
    author = "Han, Xudong  and Shen, Aili  and Cohn, Trevor  and Baldwin, Timothy  and Frermann, Lea",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.6",
    pages = "68--81",
    abstract = "Mitigating bias in training on biased datasets is an important open problem. Several techniques have been proposed, however the typical evaluation regime is very limited, considering very narrow data conditions. For instance, the effect of target class imbalance and stereotyping is under-studied. To address this gap, we examine the performance of various debiasing methods across multiple tasks, spanning binary classification (Twitter sentiment), multi-class classification (profession prediction), and regression (valence prediction). Through extensive experimentation, we find that data conditions have a strong influence on relative model performance, and that general conclusions cannot be drawn about method efficacy when evaluating only on standard datasets, as is current practice in fairness research.",
}
@inproceedings{baldini-etal-2022-fairness,
    title = "Your fairness may vary: Pretrained language model fairness in toxic text classification",
    author = "Baldini, Ioana  and Wei, Dennis  and Natesan Ramamurthy, Karthikeyan  and Singh, Moninder  and Yurochkin, Mikhail",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.176",
    doi = "10.18653/v1/2022.findings-acl.176",
    pages = "2245--2262",
    abstract = "The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.",
}
@inproceedings{chakraborty-etal-2023-counter,
    title = "Counter {T}uring Test ({CT}2): {AI}-Generated Text Detection is Not as Easy as You May Think - Introducing {AI} Detectability Index ({ADI})",
    author = "Chakraborty, Megha  and Tonmoy, S.M Towhidul Islam  and Zaman, S M Mehedi  and Gautam, Shreya  and Kumar, Tanay  and Sharma, Krish  and Barman, Niyar  and Gupta, Chandan  and Jain, Vinija  and Chadha, Aman  and Sheth, Amit  and Das, Amitava",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.136",
    doi = "10.18653/v1/2023.emnlp-main.136",
    pages = "2206--2239",
    abstract = "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that {``}if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright{''}. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.",
}
@inproceedings{uchendu-etal-2021-turingbench-benchmark,
    title = "{TURINGBENCH}: A Benchmark Environment for {T}uring Test in the Age of Neural Text Generation",
    author = "Uchendu, Adaku  and Ma, Zeyu  and Le, Thai  and Zhang, Rui  and Lee, Dongwon",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.172",
    doi = "10.18653/v1/2021.findings-emnlp.172",
    pages = "2001--2016",
    abstract = "Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called {''}Turing Test{''} problem for neural text generation methods. In this work, we present the TURINGBENCH benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels Human, GPT-1, GPT-2{\_}small, GPT-2{\_}medium, GPT-2{\_}large,GPT-2{\_}xl, GPT-2{\_}PyTorch, GPT-3, GROVER{\_}base, GROVER{\_}large, GROVER{\_}mega, CTRL, XLM, XLNET{\_}base, XLNET{\_}large, FAIR{\_}wmt19, FAIR{\_}wmt20, TRANSFORMER{\_}XL, PPLM{\_}distil, PPLM{\_}gpt2, (2) two benchmark tasks{--}i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TURINGBENCH show that GPT-3 and FAIR{\_}wmt20 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TURINGBENCH is available at: \url{https://turingbench.ist.psu.edu/}",
}
@inproceedings{lowe-etal-2017-towards,
    title = "Towards an Automatic {T}uring Test: Learning to Evaluate Dialogue Responses",
    author = "Lowe, Ryan  and Noseworthy, Michael  and Serban, Iulian Vlad  and Angelard-Gontier, Nicolas  and Bengio, Yoshua  and Pineau, Joelle",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1103",
    doi = "10.18653/v1/P17-1103",
    pages = "1116--1126",
    abstract = "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model{'}s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.",
}
@inproceedings{sugawara-tsugita-2023-degrees,
    title = "On Degrees of Freedom in Defining and Testing Natural Language Understanding",
    author = "Sugawara, Saku and Tsugita, Shun",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.861",
    doi = "10.18653/v1/2023.findings-acl.861",
    pages = "13625--13649",
    abstract = "Natural language understanding (NLU) studies often exaggerate or underestimate the capabilities of systems, thereby limiting the reproducibility of their findings. These erroneous evaluations can be attributed to the difficulty of defining and testing NLU adequately. In this position paper, we reconsider this challenge by identifying two types of researcher degrees of freedom. We revisit Turing{'}s original interpretation of the Turing test and reveal that an effective test of NLU does not provide an operational definition; it merely provides inductive evidence that the test subject understands the language sufficiently well to meet stakeholder objectives. In other words, stakeholders are free to arbitrarily define NLU through their objectives. To use the test results as inductive evidence, stakeholders must carefully assess if the interpretation of test scores is valid or not. However, designing and using NLU tests involve other degrees of freedom, such as specifying target skills and defining evaluation metrics. As a result, achieving consensus among stakeholders becomes difficult. To resolve this issue, we propose a validity argument, which is a framework comprising a series of validation criteria across test components. By demonstrating that current practices in NLU studies can be associated with those criteria and organizing them into a comprehensive checklist, we prove that the validity argument can serve as a coherent guideline for designing credible test sets and facilitating scientific communication.",
}
@inproceedings{gao-emami-2023-turing,
    title = "The {T}uring Quest: Can Transformers Make Good {NPC}s?",
    author = "Gao, Qi Chen and Emami, Ali",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-srw.17",
    doi = "10.18653/v1/2023.acl-srw.17",
    pages = "93--103",
    abstract = "In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of repetition. Finally, we propose a new benchmark, called The Turing Quest, which we use to show that the pipeline, when applied to GPT-3, can generate for a variety of game genres and contexts, NPC scripts that can fool judges in thinking they have been written by humans. We believe that these findings can greatly benefit both the gaming industry and its global community of users, since many current games continue to base their NPCs on manually-curated scripts that are resource-demanding and may curb the immersiveness and enjoyment of the user.",
}
@inproceedings{kathrin-blagec-2022-global-analysis,
    title = "A global analysis of metrics used for measuring performance in natural language processing",
    author = "Blagec, Kathrin and Dorffner, Georg and Moradi, Milad and Ott, Simon and Samwald, Matthias",
    booktitle = "Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP, pages 5263, Dublin, Ireland. Association for Computational Linguistics.",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlppower-1.6/",
    doi = "10.18653/v1/2022.nlppower-1.6",
    pages = "52-63",
    abstract = "Measuring the performance of natural language processing models is challenging. Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages. In the past 15 years, a wide range of alternative metrics have been proposed. However, it is unclear to what extent this has had an impact on NLP benchmarking efforts. Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository Papers with Code to enable a global and comprehensive analysis. Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models performance. Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.",
}
@inproceedings{oscar-sainz-2023-nlp-eval,
    title = "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
    author = "Sainz, Oscar and Campos, Jon and Garca-Ferrero, Iker and Etxaniz, Julen and Lopez de Lacalle, Oier and Agirre, Eneko",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.722/",
    doi = "10.18653/v1/2023.findings-emnlp.722",
    pages = "1077610787",
    abstract = "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
}
@inproceedings{pieter-delobelle-2022-bais-fairness,
    title = "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
    author = "Delobelle, Pieter and Tokpo, Ewoenam and Calders, Toon and Berendt, Bettina",
    booktitle = " Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = july,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.122/",
    doi = "10.18653/v1/2022.naacl-main.122",
    pages = "16931706",
    abstract = "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify bias and fairness in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
}

@inproceedings{elizabeth-clark-2021-human-eval,
    title = "All Thats Human Is Not Gold: Evaluating Human Evaluation of Generated Text",
    author = "Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan,Suchin and  Smith, Noah A.",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = august,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "hhttps://aclanthology.org/2021.acl-long.565",
    doi = "10.18653/v1/2021.acl-long.565",
    pages = "72827296",
    abstract = "Human evaluations are typically considered the gold standard in natural language generation, but as models fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models..",
}